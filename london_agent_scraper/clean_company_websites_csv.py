#!/usr/bin/env python3
"""Utility to clean scraper_company_websites output CSV files."""

from __future__ import annotations

import argparse
import csv
import re
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable, Tuple

EMAIL_PATTERN = re.compile(r"^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$")
ZERO_WIDTH_RE = re.compile(r"[\u200b-\u200d\ufeff]")
PLACEHOLDER_DOMAINS = {
    "example.com",
    "example.net",
    "example.org",
    "example.co.uk",
}
UNLIKELY_TLDS = {
    "jpg",
    "jpeg",
    "png",
    "gif",
    "svg",
    "webp",
}

_DENY_KEYWORDS = (
    "press",
    "media",
    "recruitment",
    "careers",
    "jobs",
    "privacy",
    "background",
)
_DENY_LITERAL = ("replytoaddress",)
_DENY_PATTERNS = (
    re.compile(r"no[-_]?reply"),
    re.compile(r"do[-_]?not[-_]?reply"),
)
_PREFERRED_KEYWORDS = (
    "info",
    "contact",
    "enquiries",
    "enquiry",
    "sales",
    "lettings",
    "customercare",
    "customerservice",
    "customer-care",
    "customer-service",
    "office",
    "hello",
)
INVALID_RATIO_THRESHOLD = 0.30

MISSING_EMAIL_REASON = "missing-email"
INVALID_FORMAT_REASON = "invalid-format"
MAILBOX_DENIED_REASON = "mailbox-denied"


@dataclass
class CleaningStats:
    total_rows: int = 0
    missing_email: int = 0
    invalid_email: int = 0
    mailbox_denied: int = 0
    duplicates_dropped: int = 0


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "Filter a scraped company websites CSV to remove invalid emails and duplicates."
        )
    )
    parser.add_argument(
        "input_csv",
        type=Path,
        help="Path to the CSV generated by scraper_company_websites.py",
    )
    parser.add_argument(
        "--output-csv",
        type=Path,
        default=None,
        help=(
            "Destination for the cleaned CSV. Defaults to adding '_cleaned' before the"
            " extension of the input file."
        ),
    )
    parser.add_argument(
        "--invalid-report",
        type=Path,
        default=None,
        help=(
            "Optional path to write a CSV containing rows that were discarded due to"
            " blank/invalid emails."
        ),
    )
    return parser.parse_args()


def normalise(value: str) -> str:
    """Trim whitespace and strip common noise from scraped values."""

    cleaned = value.strip().strip("\"'")
    if not cleaned:
        return ""

    cleaned = re.sub(r"(?i)^mailto:", "", cleaned)

    # Remove stray unicode escape fragments that sometimes leak from inline scripts
    cleaned = re.sub(r"u00[0-9a-fA-F]{2}", "", cleaned)

    # Angle brackets occasionally wrap mailto links – drop them for validation
    cleaned = cleaned.strip("<>")

    cleaned = ZERO_WIDTH_RE.sub("", cleaned)

    return cleaned


def email_is_valid(email: str) -> bool:
    if not email:
        return False

    match = EMAIL_PATTERN.match(email)
    if match is None:
        return False

    _, domain = email.rsplit("@", 1)
    domain_lower = domain.lower()
    if domain_lower in PLACEHOLDER_DOMAINS:
        return False

    tld = domain_lower.rsplit(".", 1)[-1]
    if tld in UNLIKELY_TLDS:
        return False

    return True


def _split_local_part(local_part: str) -> list[str]:
    tokens = re.split(r"[._+\-]", local_part)
    return [token for token in tokens if token]


def _keyword_match(local_part: str, keyword: str) -> bool:
    if local_part.startswith(keyword) or local_part.endswith(keyword):
        return True
    return keyword in _split_local_part(local_part)


def assess_mailbox(email: str) -> tuple[bool, int]:
    """Return (allowed, score) for an email address based on heuristics."""

    try:
        local_part, _ = email.rsplit("@", 1)
    except ValueError:
        return False, 0

    local_part = local_part.lower()

    for keyword in _DENY_KEYWORDS:
        if _keyword_match(local_part, keyword):
            return False, 0

    for literal in _DENY_LITERAL:
        if literal in local_part:
            return False, 0

    for pattern in _DENY_PATTERNS:
        if pattern.search(local_part):
            return False, 0

    score = 0
    condensed = local_part.replace(" ", "")
    tokens = _split_local_part(condensed)
    for keyword in _PREFERRED_KEYWORDS:
        simplified = keyword.replace("-", "")
        if condensed.startswith(simplified) or simplified in tokens:
            score += 1

    if "customer" in tokens and ("care" in tokens or "service" in tokens):
        score += 1

    return True, score


def derive_paths(input_csv: Path, output_csv: Path | None, invalid_report: Path | None) -> Tuple[Path, Path | None]:
    if output_csv is None:
        output_csv = input_csv.with_name(
            f"{input_csv.stem}_cleaned{input_csv.suffix}"
        )
    if invalid_report is None:
        return output_csv, None
    return output_csv, invalid_report


def clean_rows(rows: Iterable[dict]) -> Tuple[list[dict], list[dict], CleaningStats]:
    seen: set[Tuple[str, str]] = set()
    cleaned_with_scores: list[tuple[int, dict]] = []
    rejected: list[dict] = []
    stats = CleaningStats()

    for row in rows:
        stats.total_rows += 1
        company = normalise(row.get("company_name", ""))
        website = normalise(row.get("website", ""))
        email = normalise(row.get("email", ""))

        if not email:
            stats.missing_email += 1
            rejected.append(
                {
                    "company_name": company,
                    "website": website,
                    "email": email,
                    "invalid_reason": MISSING_EMAIL_REASON,
                }
            )
            continue

        if not email_is_valid(email):
            stats.invalid_email += 1
            rejected.append(
                {
                    "company_name": company,
                    "website": website,
                    "email": email,
                    "invalid_reason": INVALID_FORMAT_REASON,
                }
            )
            continue

        allowed, score = assess_mailbox(email)
        if not allowed:
            stats.mailbox_denied += 1
            rejected.append(
                {
                    "company_name": company,
                    "website": website,
                    "email": email,
                    "invalid_reason": MAILBOX_DENIED_REASON,
                }
            )
            continue

        key = (website.lower(), email.lower())
        if key in seen:
            stats.duplicates_dropped += 1
            continue
        seen.add(key)

        cleaned_with_scores.append(
            (
                score,
                {
                    "company_name": company,
                    "website": website,
                    "email": email,
                },
            )
        )

    cleaned_with_scores.sort(
        key=lambda item: (
            -item[0],
            item[1]["company_name"].lower(),
            item[1]["email"].lower(),
        )
    )
    cleaned = [entry for _, entry in cleaned_with_scores]

    return cleaned, rejected, stats


def write_csv(path: Path, rows: Iterable[dict]) -> None:
    rows = list(rows)
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", newline="") as handle:
        base_fields = ["company_name", "website", "email"]
        extra_fields = sorted(
            {
                field
                for row in rows
                for field in row.keys()
                if field not in base_fields
            }
        )
        fieldnames = base_fields + extra_fields
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        writer.writeheader()
        for row in rows:
            writer.writerow(row)


def main() -> None:
    args = parse_args()
    output_csv, invalid_report = derive_paths(args.input_csv, args.output_csv, args.invalid_report)

    with args.input_csv.open(newline="") as handle:
        reader = csv.DictReader(handle)
        cleaned, rejected, stats = clean_rows(reader)

    invalid_count = len(rejected)
    considered_rows = stats.total_rows - stats.missing_email
    invalid_with_email = stats.invalid_email + stats.mailbox_denied
    ratio = (invalid_with_email / considered_rows) if considered_rows else 0

    write_csv(output_csv, cleaned)

    if invalid_report is not None and rejected:
        write_csv(invalid_report, rejected)

    rejection_summary = []
    if stats.missing_email:
        rejection_summary.append(f"missing-email={stats.missing_email}")
    if stats.invalid_email:
        rejection_summary.append(f"invalid-format={stats.invalid_email}")
    if stats.mailbox_denied:
        rejection_summary.append(f"mailbox-denied={stats.mailbox_denied}")

    if invalid_report is None:
        if rejected:
            summary = ", ".join(rejection_summary) if rejection_summary else str(len(rejected))
            print(f"Discarded {len(rejected)} rows ({summary}).")
        print(f"Wrote {len(cleaned)} cleaned rows to {output_csv}")
    else:
        print(
            f"Wrote {len(cleaned)} cleaned rows to {output_csv} and"
            f" {len(rejected)} rejected rows to {invalid_report}"
        )
        if rejection_summary:
            print("Rejection breakdown: " + ", ".join(rejection_summary))

    if stats.duplicates_dropped:
        print(f"Skipped {stats.duplicates_dropped} duplicate rows based on website/email pairs.")

    if ratio > INVALID_RATIO_THRESHOLD:
        print("::error:: High invalid rate – manual review required.")
        sys.exit(1)


if __name__ == "__main__":
    main()
